{
  "dxt_version": "0.1",
  "name": "deepseek-r1",
  "display_name": "Deepseek R1",
  "version": "1.0.0",
  "description": "A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)",
  "long_description": "",
  "author": {
    "name": "66julienmartin",
    "url": "https://github.com/66julienmartin"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/66julienmartin/MCP-server-Deepseek_R1"
  },
  "homepage": "https://github.com/66julienmartin/MCP-server-Deepseek_R1",
  "screenshots": [],
  "server": {
    "type": "node",
    "entry_point": "",
    "mcp_config": {
      "command": "npx",
      "args": [
        "-y",
        "https://github.com/66julienmartin/MCP-server-Deepseek_R1"
      ],
      "env": {
        "DEEPSEEK_API_KEY": "${user_config.DEEPSEEK_API_KEY}"
      }
    }
  },
  "tools": [
    {
      "name": "deepseek_r1",
      "description": "Generate text using DeepSeek R1 model",
      "inputSchema": {
        "type": "object",
        "properties": {
          "prompt": {
            "type": "string",
            "description": "Input text for DeepSeek"
          },
          "max_tokens": {
            "type": "number",
            "description": "Maximum tokens to generate (default: 8192)",
            "minimum": 1,
            "maximum": 8192
          },
          "temperature": {
            "type": "number",
            "description": "Sampling temperature (default: 0.2)",
            "minimum": 0,
            "maximum": 2
          }
        },
        "required": [
          "prompt"
        ]
      }
    }
  ],
  "prompts": [],
  "tools_generated": true,
  "keywords": [
    "AI Systems",
    "LLM",
    "Deepseek"
  ],
  "license": "MIT",
  "user_config": {
    "DEEPSEEK_API_KEY": {
      "type": "string",
      "description": "API key for authenticating with the Deepseek service.",
      "sensitive": true,
      "title": "DEEPSEEK_API_KEY",
      "required": true
    }
  }
}